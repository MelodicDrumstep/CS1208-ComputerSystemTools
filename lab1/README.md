# Tiny Beautiful Things——计算机系统工具与操作课程作业

这篇文档介绍了我所使用的数据转换方法和宇航员所需的数据恢复方法。

## 数据转换方法: 分为数据提取方法和数据压缩方法
### 数据提取方法：使用Github上的一个开源爬虫程序
这是我找了很久之后发现的一个开源程序，在提取微信公众号文章内容方面表现最好，尤其是生成的文件格式非常好看。Github链接如下：https://github.com/luzhixing12345/WeChat-official-account-spider
B站还有配套教程视频：https://www.bilibili.com/video/BV1La411B7Qd/?spm_id_from=333.337.search-card.all.click

大致操作流程即为：
1.安装依赖（这一步我卡了很久，首先是没有先执行pip freeze > requirements.txt，其次是requirements.txt没有写成相对路径./requirements.txt，实际上Github上的README文件在这一部分讲解不够详细）

2.根据自己电脑上谷歌浏览器的版本安装chromedriver，将其复制到chrome文件夹和python解释器文件夹中并配置环境变量。由于我之前配置过Python的环境变量，所以这部分对我来说比较简单。
3.执行python GUI.py，输入网址，在文件夹中找到生成的word文件。

在这一方法中，我还学到了可以在网页界面按下F12按钮，查看每个元素的位置。不过并没有用到最后的数据转换方法中。


#### 数据提取中尝试过的方案（这部分不用给宇航员看）
1.进入网页，使用ctrl + c / v，讲内容复制进word，再将多余的部分删除。
弃用原因：不具备自动化，而且格式不好看。若网页过多，则地球方任务量过大


2.使用爬虫技术，自己编写一个Python程序，使用beautifulsoup模块对网页进行信息提取。
弃用原因：我一开始一直想使用这个方法，也看了很多教程学习。比如youtube视频 https://www.youtube.com/watch?v=gRLHr664tXA 和官方教程 https://beautiful-soup-4.readthedocs.io/en/latest/ ，但折腾了好久还是写不好python程序，主要原因在于对javascript和Html完全不熟悉，而且微信公众号文章的html源码有4000多行，每次抓取都会抓错或者失败（微信公众号网页的源码好复杂），受限于短期技术原因所以放弃该方案，等我对html有更多了解了再把它写完。


3.使用第三方软件，如“八爪鱼信息采集平台”。
弃用原因：这个方案非常的简便，思维量几乎为零，自动化效果也很好，但是第三方软件往往需要付费，且自己学不到信息采集的知识，因此弃用。但如果地球方时间不充裕，完全可以采用第三方软件。

4.另外还寻求了chatGPT的帮助，但是chatGPT并没有给出什么有用的信息，不如上网搜索。

### 数据压缩方法
我直接选择使用课堂上讲过的7-zip软件，这个软件操作简单且好用。我们直接选中我们想要压缩的文件，然后右键点击7-zip->添加到XXX.7z文件或者XXX.zip文件。.7z文件压缩效率更高，不过由于此处文件大小较小，所以差别不明显。我上交的文件中同时包含.7z文件和.zip文件。

## 数据恢复方法
宇航员需要下载安装7-zip,然后选中这些文件，右键点击7-zip->提取文件。这样即可提取出.docx文件
